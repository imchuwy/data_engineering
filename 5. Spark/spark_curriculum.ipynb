{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PySpark mini-curriculum\n",
        "## Goals\n",
        "- Create a SparkSession and read CSV/Parquet\n",
        "- DataFrame operations: select/withColumn/filter/groupBy\n",
        "- UDFs vs built-ins; when to avoid UDFs\n",
        "- Joins, window functions, and aggregations\n",
        "- Spark MLlib basics: feature assemblers, estimators/transformers, pipelines\n",
        "- Performance tips: partitions, caching, explain plans\n",
        "\n",
        "## Step-by-step path\n",
        "1) Setup & mindset\n",
        "   - Install Spark locally (or use Databricks/EMR); set `SPARK_HOME`, test `SparkSession.builder.getOrCreate()`.\n",
        "   - Favor built-in functions over Python UDFs; keep schemas explicit to avoid surprises.\n",
        "2) SparkSession + IO\n",
        "   - Create a session with sensible defaults (e.g., `master=\"local[*]\"`, `appName`).\n",
        "   - Read CSV/Parquet with `spark.read.option(...).csv/parquet`; check schema with `printSchema()`; set `inferSchema` cautiously.\n",
        "3) DataFrame basics\n",
        "   - Column ops: `select`, `withColumn`, `filter`/`where`, `drop`, `distinct`.\n",
        "   - Expressions: use `pyspark.sql.functions` (e.g., `col`, `lit`, `when`, string/date functions) instead of UDFs when possible.\n",
        "4) Aggregations\n",
        "   - Grouped stats with `groupBy().agg(...)`; handle nulls with `na.fill`/`drop`.\n",
        "   - Use `approx_count_distinct`, `percentile_approx` for scalable metrics.\n",
        "5) Joins\n",
        "   - Inner/left/anti/semi joins; beware duplicated keys changing row counts.\n",
        "   - Control join strategy hints (`broadcast`, `merge`) and confirm with `explain()`.\n",
        "6) Window functions\n",
        "   - Define `Window.partitionBy(...).orderBy(...)`; apply `row_number`, `lag/lead`, running sums.\n",
        "   - Use frame specs (`rowsBetween`) for rolling windows; confirm ordering determinism.\n",
        "7) Data cleaning & types\n",
        "   - Cast columns explicitly; parse timestamps with formats/timezones.\n",
        "   - Handle skewed categories and outliers; consider bucketing or clipping before heavy joins.\n",
        "8) Spark MLlib pipelines\n",
        "   - Assemble features with `VectorAssembler`; handle categoricals with `StringIndexer` + `OneHotEncoder`.\n",
        "   - Build `Pipeline` with transformers + estimators (e.g., `LogisticRegression`, `RandomForestClassifier`).\n",
        "9) Evaluation & tuning\n",
        "   - Split with `randomSplit`; use `BinaryClassificationEvaluator`/`RegressionEvaluator`.\n",
        "   - Tune with `CrossValidator` or `TrainValidationSplit`; log `avgMetrics` and best params.\n",
        "10) Performance & stability\n",
        "    - Inspect physical plans with `explain(mode=\"formatted\")`; avoid wide shuffles.\n",
        "    - Repartition/coalesce intentionally, cache only when reused; monitor via Spark UI; write Parquet with partitioning and `mode=\"overwrite\"`.\n",
        "\n",
        "## Suggested exercises\n",
        "- Start a local Spark session, read a CSV, and compute grouped stats + a filtered subset.\n",
        "- Add a window function (e.g., `row_number` over partition) and validate ordering and counts.\n",
        "- Build a small ML pipeline (StringIndexer -> OneHotEncoder -> VectorAssembler -> LogisticRegression), tune 2-3 hyperparameters, and evaluate on a holdout.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}